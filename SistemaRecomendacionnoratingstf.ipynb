{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5u4C2LQdwoT"
      },
      "source": [
        "#Sistema de recomendación TF sin ratings\n",
        "\n",
        "Este ejemplo es muy similar al del archivo modelado en TF del repositorio pero en este no tendremos solo tendremos en cuenta datos de calificación positiva es decir cuya calificación sea superior a 3 estrellas. Por este motivo obviaremos mucha documentación de los pasos realizados. Leemos los datasets de tfrecords previamente desarrollados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMaUB_JHdvBR",
        "outputId": "52704267-fded-4ad6-b94c-67de101f0a2c"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "filenames = ['traindata.tfrecord']\n",
        "raw_dataset = tf.data.TFRecordDataset(filenames)\n",
        "raw_dataset_test = tf.data.TFRecordDataset(['testdata.tfrecord'])\n",
        "raw_dataset"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TFRecordDatasetV2 shapes: (), types: tf.string>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZH3RaHMtgEUV",
        "outputId": "538d3d3d-2a8d-4759-8cf9-364b60b5734b"
      },
      "source": [
        "# Create a description of the features.\n",
        "feature_description = {\n",
        "    'review_id': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
        "    'product_id': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
        "    'reviewer_id': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
        "    'stars': tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
        "    'review_body': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
        "    'review_title': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
        "    'language': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
        "    'product_category': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
        "}\n",
        "\n",
        "def _parse_function(example_proto):\n",
        "  # Parse the input `tf.train.Example` proto using the dictionary above.\n",
        "  return tf.io.parse_single_example(example_proto, feature_description)\n",
        "\n",
        "# mapper en paralelo para optimizar tiempo de ejecución\n",
        "parsed_dataset = raw_dataset.map(_parse_function)\n",
        "parsed_test_dataset = raw_dataset_test.map(_parse_function)\n",
        "\n",
        "parsed_dataset"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<MapDataset shapes: {language: (), product_category: (), product_id: (), review_body: (), review_id: (), review_title: (), reviewer_id: (), stars: ()}, types: {language: tf.string, product_category: tf.string, product_id: tf.string, review_body: tf.string, review_id: tf.string, review_title: tf.string, reviewer_id: tf.string, stars: tf.int64}>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4zrFSHPgJj9"
      },
      "source": [
        "Filtramos los datos de los datos que si deberían recomendarse\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQeIOczLgPON"
      },
      "source": [
        "import pprint\n",
        "def dataset_filter_stars(ds):\n",
        "  return ds.filter(lambda x: x['stars'] > 4)\n",
        "\n",
        "\n",
        "parsed_dataset = parsed_dataset.apply(dataset_filter_stars)\n",
        "\n",
        "parsed_dataset = parsed_dataset.map(lambda x: {\n",
        "    'reviewer_id':x['reviewer_id'], \n",
        "    'product_id':x['product_id']\n",
        "})\n",
        "\n",
        "parsed_test_dataset = parsed_test_dataset.map(lambda x: {\n",
        "    'reviewer_id':x['reviewer_id'], \n",
        "    'product_id':x['product_id']\n",
        "})"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32v1bceEs99q",
        "outputId": "c4b87509-05a5-4ac9-f3a4-436344fb26df"
      },
      "source": [
        "parsed_test_dataset"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<MapDataset shapes: {reviewer_id: (), product_id: ()}, types: {reviewer_id: tf.string, product_id: tf.string}>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCHwZuU1nhG3"
      },
      "source": [
        "train = parsed_dataset\n",
        "test = parsed_test_dataset"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toRBJw-untp5",
        "outputId": "02a5d677-7b6f-49de-d024-1dcb6f9fe7fa"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "products = parsed_dataset.batch(1000).map(lambda x: x[\"product_id\"])\n",
        "user_ids = parsed_dataset.batch(1000).map(lambda x: x[\"reviewer_id\"])\n",
        "\n",
        "unique_products = np.unique(np.concatenate(list(products)))\n",
        "unique_user_ids = np.unique(np.concatenate(list(user_ids)))\n",
        "\n",
        "unique_products[:10]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'product_de_0000060', b'product_de_0000070',\n",
              "       b'product_de_0000122', b'product_de_0000192',\n",
              "       b'product_de_0000260', b'product_de_0000295',\n",
              "       b'product_de_0000322', b'product_de_0000349',\n",
              "       b'product_de_0000418', b'product_de_0000527'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAGGcmwQotz4"
      },
      "source": [
        "embedding_dimension = 32 #hay que probar valores que mejor se ajusten"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyLUibqxozpW"
      },
      "source": [
        "user_model = tf.keras.Sequential([\n",
        "  tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "      vocabulary=unique_user_ids, mask_token=None),\n",
        "  # We add an additional embedding to account for unknown tokens.\n",
        "  tf.keras.layers.Embedding(len(unique_user_ids) + 1, embedding_dimension)\n",
        "])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_dAQmB4pKHn"
      },
      "source": [
        "product_model = tf.keras.Sequential([\n",
        "  tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "      vocabulary=unique_products, mask_token=None),\n",
        "  tf.keras.layers.Embedding(len(unique_products) + 1, embedding_dimension)\n",
        "])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JutpM1h9p_RZ"
      },
      "source": [
        "#!pip install -q tensorflow-recommenders\n",
        "#!pip install -q --upgrade tensorflow-datasets\n",
        "#!pip install -q scann"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyLgkpSkqAtM"
      },
      "source": [
        "#import tensorflow_recommenders as tfrs"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5YN0SODsLAJ"
      },
      "source": [
        "tmp = parsed_test_dataset.map(lambda x: x['product_id'])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzS5Th4epX9D"
      },
      "source": [
        "import tensorflow_recommenders as tfrs\n",
        "\n",
        "metrics = tfrs.metrics.FactorizedTopK(\n",
        "  candidates= tmp.batch(128).map(product_model)\n",
        ")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzxIADzxtJYe"
      },
      "source": [
        "task = tfrs.tasks.Retrieval(\n",
        "  metrics=metrics\n",
        ")"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5wB-AwHtPfp"
      },
      "source": [
        "import os\n",
        "import pprint\n",
        "import tempfile\n",
        "\n",
        "from typing import Dict, Text\n",
        "\n",
        "\n",
        "class MovielensModel(tfrs.Model):\n",
        "\n",
        "  def __init__(self, user_model, movie_model):\n",
        "    super().__init__()\n",
        "    self.movie_model: tf.keras.Model = movie_model\n",
        "    self.user_model: tf.keras.Model = user_model\n",
        "    self.task: tf.keras.layers.Layer = task\n",
        "\n",
        "  def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
        "    # We pick out the user features and pass them into the user model.\n",
        "    user_embeddings = self.user_model(features[\"reviewer_id\"])\n",
        "    # And pick out the movie features and pass them into the movie model,\n",
        "    # getting embeddings back.\n",
        "    positive_movie_embeddings = self.movie_model(features[\"product_id\"])\n",
        "\n",
        "    # The task computes the loss and the metrics.\n",
        "    return self.task(user_embeddings, positive_movie_embeddings)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFaJWigjtaSY"
      },
      "source": [
        "model = MovielensModel(user_model, product_model)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMcsyFW7wXlG",
        "outputId": "d97715cc-f071-4564-fc63-f41efd1fae99"
      },
      "source": [
        "train"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<MapDataset shapes: {reviewer_id: (), product_id: ()}, types: {reviewer_id: tf.string, product_id: tf.string}>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNayr1SttxGu"
      },
      "source": [
        "cached_train = train.shuffle(100_000).batch(8192).cache()\n",
        "cached_test = test.batch(4100).cache()"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRoeVR1pt8z_",
        "outputId": "458a73c9-68a2-4dfa-8aeb-0b8090a74299"
      },
      "source": [
        "model.fit(cached_train, epochs=3)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
            "8/8 [==============================] - 61s 6s/step - factorized_top_k/top_1_categorical_accuracy: 0.0036 - factorized_top_k/top_5_categorical_accuracy: 0.0189 - factorized_top_k/top_10_categorical_accuracy: 0.0379 - factorized_top_k/top_50_categorical_accuracy: 0.1842 - factorized_top_k/top_100_categorical_accuracy: 0.3340 - loss: 62067.3186 - regularization_loss: 0.0000e+00 - total_loss: 62067.3186\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 45s 6s/step - factorized_top_k/top_1_categorical_accuracy: 0.5324 - factorized_top_k/top_5_categorical_accuracy: 0.8353 - factorized_top_k/top_10_categorical_accuracy: 0.9133 - factorized_top_k/top_50_categorical_accuracy: 0.9880 - factorized_top_k/top_100_categorical_accuracy: 0.9923 - loss: 61950.3077 - regularization_loss: 0.0000e+00 - total_loss: 61950.3077\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 45s 6s/step - factorized_top_k/top_1_categorical_accuracy: 0.9502 - factorized_top_k/top_5_categorical_accuracy: 0.9989 - factorized_top_k/top_10_categorical_accuracy: 0.9998 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 61809.3472 - regularization_loss: 0.0000e+00 - total_loss: 61809.3472\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f46d94aced0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQqaIa3hyFRk",
        "outputId": "0cfef63c-92ec-4ab1-8081-7f3cf7df9721"
      },
      "source": [
        "model.evaluate(cached_test, return_dict=True)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 5s 2s/step - factorized_top_k/top_1_categorical_accuracy: 1.3333e-04 - factorized_top_k/top_5_categorical_accuracy: 6.6667e-04 - factorized_top_k/top_10_categorical_accuracy: 0.0013 - factorized_top_k/top_50_categorical_accuracy: 0.0073 - factorized_top_k/top_100_categorical_accuracy: 0.9724 - loss: 29800.4115 - regularization_loss: 0.0000e+00 - total_loss: 29800.4115\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'factorized_top_k/top_100_categorical_accuracy': 0.9724000096321106,\n",
              " 'factorized_top_k/top_10_categorical_accuracy': 0.0013333333190530539,\n",
              " 'factorized_top_k/top_1_categorical_accuracy': 0.00013333333481568843,\n",
              " 'factorized_top_k/top_50_categorical_accuracy': 0.007333333138376474,\n",
              " 'factorized_top_k/top_5_categorical_accuracy': 0.0006666666595265269,\n",
              " 'loss': 27647.16796875,\n",
              " 'regularization_loss': 0,\n",
              " 'total_loss': 27647.16796875}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFI0PebbQW5S"
      },
      "source": [
        "## Conclusiones\n",
        "Se observa que este acercamiento no es muy bueno para la recomendación porque hay demasiados datos nulos y no pueden y el algoritmo intenta asignar un producto sin tener en cuenta muchas cosas."
      ]
    }
  ]
}