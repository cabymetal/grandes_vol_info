{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Isfss2Z4eKuK"
   },
   "source": [
    "# Lectura e instalación de librerías\n",
    "\n",
    "En esta sección se realiza la configuración para tener acceso a los datos que se van a ejecutar en el proceso, para el **análisis descriptivo** se utilizan solo 2 librerías y pandas porque la totalidad de los datos hace posible manejarlo en memoria de manera eficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PY3dzDdXUy9D"
   },
   "outputs": [],
   "source": [
    "!pip install boto3\n",
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "s3 = boto3.resource('s3', aws_access_key_id = '',\n",
    "                    aws_secret_access_key='',\n",
    "                    aws_session_token='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lBR8QXyLeuJH"
   },
   "source": [
    "## Análisis descriptivo (parte 1)\n",
    "realizamos un método que nos permite descargar los datos y ver los datos de las variables más relevantes que se van a utilizar por ejemplo en la primera fila:\n",
    "nombre del archivo, cantidad de filas y cantidad de columnas, cantidad de productos y cantidad de usuarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qnGXJgciU8dE"
   },
   "outputs": [],
   "source": [
    "for my_bucket_object in s3.Bucket('big-data-mining').objects.all():\n",
    "    if 'dataset' in my_bucket_object.key:\n",
    "      s3.Bucket('big-data-mining').download_file(my_bucket_object.key, my_bucket_object.key.split('/')[2])\n",
    "      df = pd.read_json(my_bucket_object.key.split('/')[2], lines=True)\n",
    "      descriptivo = [my_bucket_object.key.split('/')[2],\n",
    "                     df.shape,\n",
    "        df['product_id'].drop_duplicates().shape[0],\n",
    "        df['reviewer_id'].drop_duplicates().shape[0]]\n",
    "\n",
    "      print(descriptivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ntG-l4_sG1A",
    "outputId": "0e222af1-c406-4856-f5a2-23280ef49d2b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30,)"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['product_category'].drop_duplicates().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "heTjPw61fuxL"
   },
   "source": [
    "## Análisis descriptivo Parte 2\n",
    "\n",
    "En este caso se realiza un conteo de estrellas donde se observa que todos los archivos tienen exactamente la misma cantidad de estrellas y su disrtribución es exactamente la misma para todos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ScWTKD2yau7H"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "for data in os.listdir('.'):\n",
    "  if 'dataset' in data:\n",
    "    print(data)\n",
    "    df = pd.read_json(data, lines=True)\n",
    "    print(df.groupby('stars').count())\n",
    "    print(df['stars'].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xFNmjD9UglqO"
   },
   "source": [
    "### columnas a utilizar\n",
    "\n",
    "Serán el producto, el usuario y la calificación (product_id, reviewer_id y stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JMisWC3JVnl3",
    "outputId": "fee0c314-23f4-44b7-86b3-a89d5f1c2f4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body',\n",
      "       'review_title', 'language', 'product_category'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RCXr3Ig9EsuH"
   },
   "source": [
    "# Lectura de JSON en TensorFlow\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "urUaspgoa5EZ"
   },
   "source": [
    "##Primero (Helper function to read json files)\n",
    "Definimos una función que se encarga de leer los archivos json que obtenemos en la sección anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "XOOwdsD5E8Zq"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os \n",
    "\n",
    "def read_json_files(pattern):\n",
    "  for jsonfile in glob.glob(pattern):\n",
    "    with open('./' + jsonfile) as tmpfilepointer:\n",
    "      for line in tmpfilepointer:\n",
    "        yield tmpfilepointer.readline()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TqDJGmkL3sMi"
   },
   "source": [
    "##Segundo (Creación de un archivo TFRecord)\n",
    "\n",
    "Con la función generadora podemos crear un tipo de dato `tf.train.Example`, para generar tfRecords a partir de allí. Tambien utilizamos los helpers functions de la documentación oficial para castear los tipos de datos que estamos trabajando a un valor consumible por un TFRecord\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pUMCEXPz5oll"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "\n",
    "# The following functions can be used to convert a value to a type compatible\n",
    "# with tf.train.Example.\n",
    "\n",
    "def _bytes_feature(value):\n",
    "  \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "  if isinstance(value, type(tf.constant(0))):\n",
    "    value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "  \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "  \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def generate_tf_record(generator, tfrecord_filename):\n",
    "  tfrecord_writer = tf.io.TFRecordWriter(tfrecord_filename)\n",
    "\n",
    "  for line in generator:\n",
    "    try:\n",
    "      json_object = json.loads(line)\n",
    "      sample_tensorflowdata = tf.train.Example(features=tf.train.Features(feature={\n",
    "          'review_id':_bytes_feature(json_object['review_id'].encode('utf-8')),\n",
    "          'product_id':_bytes_feature(json_object['product_id'].encode('utf-8')),\n",
    "          'reviewer_id':_bytes_feature(json_object['reviewer_id'].encode('utf-8')),\n",
    "          'stars':_int64_feature(int(json_object['stars'])),\n",
    "          'review_body':_bytes_feature(json_object['review_body'].encode('utf-8')),\n",
    "          'review_title':_bytes_feature(json_object['review_title'].encode('utf-8')),\n",
    "          'language':_bytes_feature(json_object['language'].encode('utf-8')),\n",
    "          'product_category':_bytes_feature(json_object['product_category'].encode('utf-8')),\n",
    "      }))\n",
    "\n",
    "      tfrecord_writer.write(sample_tensorflowdata.SerializeToString())\n",
    "    except Exception as e:\n",
    "      print(f\"Error en línea: {line}\")\n",
    "      print(e)\n",
    "      continue\n",
    "  \n",
    "  tfrecord_writer.close()\n",
    "  \n",
    "\n",
    "generator = read_json_files('*_train.json')\n",
    "generate_tf_record(generator, 'traindata.tfrecord')\n",
    "\n",
    "generator = read_json_files('*_test.json')\n",
    "generate_tf_record(generator, 'testdata.tfrecord')\n",
    "    \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IgHiOGvUVcRq"
   },
   "source": [
    "de esta manera generamos el archivo ***data.record*** que esta serializado para que sea más fácil su consumo en la librería\n",
    "\n",
    "## Tercero (Lectura de archivo TFRecord)\n",
    "\n",
    "Solo para el ejercicio decidimos utilizar la lectura y la escritura de los TFRecords porque toda la información se encuentra en el mismo ambiente, estos archivos solo son necesarios para transporte de información o para generarla utilizando otra estrategia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HIusrmT3Vsvj",
    "outputId": "01a80945-debd-42d5-9ef2-d759d735af77"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TFRecordDatasetV2 shapes: (), types: tf.string>"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames = ['traindata.tfrecord']\n",
    "raw_dataset = tf.data.TFRecordDataset(filenames)\n",
    "raw_dataset_test = tf.data.TFRecordDataset(['testdata.tfrecord'])\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wh09VbUAYLKx"
   },
   "source": [
    "despues podemos explorar los datos como se realizaría en tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4weeiJeLYSL9",
    "outputId": "9a01ae38-4bcb-4c64-d7a2-3be14b745494"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Tensor: shape=(), dtype=string, numpy=b'\\n\\x9e\\x02\\n$\\n\\nproduct_id\\x12\\x16\\n\\x14\\n\\x12product_de_0678997\\n5\\n\\x0breview_body\\x12&\\n$\\n\"In der Lieferung war nur Ein Akku!\\n\\x12\\n\\x08language\\x12\\x06\\n\\x04\\n\\x02de\\n,\\n\\x0creview_title\\x12\\x1c\\n\\x1a\\n\\x18EINS statt ZWEI Akkus!!!\\n&\\n\\x0breviewer_id\\x12\\x17\\n\\x15\\n\\x13reviewer_de_0783625\\n\\x0e\\n\\x05stars\\x12\\x05\\x1a\\x03\\n\\x01\\x01\\n\\x1b\\n\\treview_id\\x12\\x0e\\n\\x0c\\n\\nde_0559494\\n(\\n\\x10product_category\\x12\\x14\\n\\x12\\n\\x10home_improvement'>\n",
      "<tf.Tensor: shape=(), dtype=string, numpy=b'\\n\\xc3\\x03\\n&\\n\\x0breviewer_id\\x12\\x17\\n\\x15\\n\\x13reviewer_de_0836478\\n\\x0e\\n\\x05stars\\x12\\x05\\x1a\\x03\\n\\x01\\x01\\n!\\n\\x10product_category\\x12\\r\\n\\x0b\\n\\tdrugstore\\n\\x1b\\n\\treview_id\\x12\\x0e\\n\\x0c\\n\\nde_0477884\\n$\\n\\nproduct_id\\x12\\x16\\n\\x14\\n\\x12product_de_0719501\\n\\xe7\\x01\\n\\x0breview_body\\x12\\xd7\\x01\\n\\xd4\\x01\\n\\xd1\\x01Dachte, das w\\xc3\\xa4ren einfach etwas festere Binden, vielleicht gr\\xc3\\xb6\\xc3\\x9fere Always. Aber die Verpackung ist derartig riesig - wie als h\\xc3\\xa4tte man einen riesigen Karton Windeln gekauft... nicht das, was ich wollte ;-)\\n\\x12\\n\\x08language\\x12\\x06\\n\\x04\\n\\x02de\\n%\\n\\x0creview_title\\x12\\x15\\n\\x13\\n\\x11Zu viel des Guten'>\n",
      "<tf.Tensor: shape=(), dtype=string, numpy=b\"\\n\\xff\\x03\\n&\\n\\x0breviewer_id\\x12\\x17\\n\\x15\\n\\x13reviewer_de_0917477\\n\\x0e\\n\\x05stars\\x12\\x05\\x1a\\x03\\n\\x01\\x01\\n!\\n\\x10product_category\\x12\\r\\n\\x0b\\n\\tdrugstore\\n\\x1b\\n\\treview_id\\x12\\x0e\\n\\x0c\\n\\nde_0146666\\n$\\n\\nproduct_id\\x12\\x16\\n\\x14\\n\\x12product_de_0935874\\n\\xa8\\x02\\n\\x0breview_body\\x12\\x98\\x02\\n\\x95\\x02\\n\\x92\\x02Schmeckt einfach nicht gut, es kostet ziemlich \\xc3\\x9cberwindung es zu trinken und ist einfach gar nichts f\\xc3\\xbcr mich! Das einige was ginge w\\xc3\\xa4re es mit Joghurt zu mischen aber das war's denn auch schon! Mein Mann meinte es schmeckt nach Penicillin und irgendwie stimmt das auch...\\n\\x12\\n\\x08language\\x12\\x06\\n\\x04\\n\\x02de\\n \\n\\x0creview_title\\x12\\x10\\n\\x0e\\n\\x0cNicht lecker\">\n",
      "<tf.Tensor: shape=(), dtype=string, numpy=b'\\n\\xf8\\x04\\n\\x12\\n\\x08language\\x12\\x06\\n\\x04\\n\\x02de\\n\\x1c\\n\\x0creview_title\\x12\\x0c\\n\\n\\n\\x08Fehlkauf\\n&\\n\\x0breviewer_id\\x12\\x17\\n\\x15\\n\\x13reviewer_de_0953310\\n\\x0e\\n\\x05stars\\x12\\x05\\x1a\\x03\\n\\x01\\x01\\n(\\n\\x10product_category\\x12\\x14\\n\\x12\\n\\x10home_improvement\\n\\x1b\\n\\treview_id\\x12\\x0e\\n\\x0c\\n\\nde_0833758\\n$\\n\\nproduct_id\\x12\\x16\\n\\x14\\n\\x12product_de_0115194\\n\\x9e\\x03\\n\\x0breview_body\\x12\\x8e\\x03\\n\\x8b\\x03\\n\\x88\\x03Habe diesen Artikel bestellt,war auf meiner Wunschliste um 84% reduziert, bekommen habe ich eine Madenschraube. Durch R\\xc3\\xbccksprache mit dem Verk\\xc3\\xa4ufer wurde mir mitgeteilt , das der Fehler bei Amazon liegt. Ich habe zwar mein Geld zur\\xc3\\xbcck bekommen, aber von seiten Amazon wurde mir nichts mitgeteilt. F\\xc3\\xbcr den Artikel von hansgrohe gibt es f\\xc3\\xbcnf Sterne,f\\xc3\\xbcr den Ablauf des Verkaufs Null Sterne'>\n",
      "<tf.Tensor: shape=(), dtype=string, numpy=b\"\\n\\x8b\\x02\\n&\\n\\x0breviewer_id\\x12\\x17\\n\\x15\\n\\x13reviewer_de_0033281\\n\\x0e\\n\\x05stars\\x12\\x05\\x1a\\x03\\n\\x01\\x01\\n'\\n\\x10product_category\\x12\\x13\\n\\x11\\n\\x0flawn_and_garden\\n\\x1b\\n\\treview_id\\x12\\x0e\\n\\x0c\\n\\nde_0232738\\n$\\n\\nproduct_id\\x12\\x16\\n\\x14\\n\\x12product_de_0901865\\n)\\n\\x0breview_body\\x12\\x1a\\n\\x18\\n\\x16Kaum Saat aufgegangen.\\n\\x12\\n\\x08language\\x12\\x06\\n\\x04\\n\\x02de\\n&\\n\\x0creview_title\\x12\\x16\\n\\x14\\n\\x12Schlechte Mischung\">\n"
     ]
    }
   ],
   "source": [
    "for raw_record in raw_dataset.take(5):\n",
    "  print(repr(raw_record))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YHJi6zyXYlfq"
   },
   "source": [
    "como esto no nos da mucha información podemos realizar un diccionario de transformaciones de cada uno de los features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GmIjk4JdYql1",
    "outputId": "9dce2129-ad5b-4941-80dc-bbb4816ed600"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: {language: (), product_category: (), product_id: (), review_body: (), review_id: (), review_title: (), reviewer_id: (), stars: ()}, types: {language: tf.string, product_category: tf.string, product_id: tf.string, review_body: tf.string, review_id: tf.string, review_title: tf.string, reviewer_id: tf.string, stars: tf.int64}>"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a description of the features.\n",
    "feature_description = {\n",
    "    'review_id': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
    "    'product_id': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
    "    'reviewer_id': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
    "    'stars': tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
    "    'review_body': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
    "    'review_title': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
    "    'language': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
    "    'product_category': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
    "}\n",
    "\n",
    "def _parse_function(example_proto):\n",
    "  # Parse the input `tf.train.Example` proto using the dictionary above.\n",
    "  return tf.io.parse_single_example(example_proto, feature_description)\n",
    "\n",
    "parsed_dataset = raw_dataset.map(_parse_function)\n",
    "\n",
    "\n",
    "parsed_test_dataset = raw_dataset_test.map(_parse_function)\n",
    "\n",
    "parsed_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8yBk-B6vZxyW"
   },
   "source": [
    "Ahora ya podemos ver los features de la siguiente manera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QF-Pe3aFZ28d",
    "outputId": "b7cc19e3-eff8-43f5-c5ae-b1d80382e4dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'language': <tf.Tensor: shape=(), dtype=string, numpy=b'de'>, 'product_category': <tf.Tensor: shape=(), dtype=string, numpy=b'home_improvement'>, 'product_id': <tf.Tensor: shape=(), dtype=string, numpy=b'product_de_0678997'>, 'review_body': <tf.Tensor: shape=(), dtype=string, numpy=b'In der Lieferung war nur Ein Akku!'>, 'review_id': <tf.Tensor: shape=(), dtype=string, numpy=b'de_0559494'>, 'review_title': <tf.Tensor: shape=(), dtype=string, numpy=b'EINS statt ZWEI Akkus!!!'>, 'reviewer_id': <tf.Tensor: shape=(), dtype=string, numpy=b'reviewer_de_0783625'>, 'stars': <tf.Tensor: shape=(), dtype=int64, numpy=1>}\n",
      "{'language': <tf.Tensor: shape=(), dtype=string, numpy=b'de'>, 'product_category': <tf.Tensor: shape=(), dtype=string, numpy=b'drugstore'>, 'product_id': <tf.Tensor: shape=(), dtype=string, numpy=b'product_de_0719501'>, 'review_body': <tf.Tensor: shape=(), dtype=string, numpy=b'Dachte, das w\\xc3\\xa4ren einfach etwas festere Binden, vielleicht gr\\xc3\\xb6\\xc3\\x9fere Always. Aber die Verpackung ist derartig riesig - wie als h\\xc3\\xa4tte man einen riesigen Karton Windeln gekauft... nicht das, was ich wollte ;-)'>, 'review_id': <tf.Tensor: shape=(), dtype=string, numpy=b'de_0477884'>, 'review_title': <tf.Tensor: shape=(), dtype=string, numpy=b'Zu viel des Guten'>, 'reviewer_id': <tf.Tensor: shape=(), dtype=string, numpy=b'reviewer_de_0836478'>, 'stars': <tf.Tensor: shape=(), dtype=int64, numpy=1>}\n",
      "{'language': <tf.Tensor: shape=(), dtype=string, numpy=b'de'>, 'product_category': <tf.Tensor: shape=(), dtype=string, numpy=b'drugstore'>, 'product_id': <tf.Tensor: shape=(), dtype=string, numpy=b'product_de_0935874'>, 'review_body': <tf.Tensor: shape=(), dtype=string, numpy=b\"Schmeckt einfach nicht gut, es kostet ziemlich \\xc3\\x9cberwindung es zu trinken und ist einfach gar nichts f\\xc3\\xbcr mich! Das einige was ginge w\\xc3\\xa4re es mit Joghurt zu mischen aber das war's denn auch schon! Mein Mann meinte es schmeckt nach Penicillin und irgendwie stimmt das auch...\">, 'review_id': <tf.Tensor: shape=(), dtype=string, numpy=b'de_0146666'>, 'review_title': <tf.Tensor: shape=(), dtype=string, numpy=b'Nicht lecker'>, 'reviewer_id': <tf.Tensor: shape=(), dtype=string, numpy=b'reviewer_de_0917477'>, 'stars': <tf.Tensor: shape=(), dtype=int64, numpy=1>}\n",
      "{'language': <tf.Tensor: shape=(), dtype=string, numpy=b'de'>, 'product_category': <tf.Tensor: shape=(), dtype=string, numpy=b'home_improvement'>, 'product_id': <tf.Tensor: shape=(), dtype=string, numpy=b'product_de_0115194'>, 'review_body': <tf.Tensor: shape=(), dtype=string, numpy=b'Habe diesen Artikel bestellt,war auf meiner Wunschliste um 84% reduziert, bekommen habe ich eine Madenschraube. Durch R\\xc3\\xbccksprache mit dem Verk\\xc3\\xa4ufer wurde mir mitgeteilt , das der Fehler bei Amazon liegt. Ich habe zwar mein Geld zur\\xc3\\xbcck bekommen, aber von seiten Amazon wurde mir nichts mitgeteilt. F\\xc3\\xbcr den Artikel von hansgrohe gibt es f\\xc3\\xbcnf Sterne,f\\xc3\\xbcr den Ablauf des Verkaufs Null Sterne'>, 'review_id': <tf.Tensor: shape=(), dtype=string, numpy=b'de_0833758'>, 'review_title': <tf.Tensor: shape=(), dtype=string, numpy=b'Fehlkauf'>, 'reviewer_id': <tf.Tensor: shape=(), dtype=string, numpy=b'reviewer_de_0953310'>, 'stars': <tf.Tensor: shape=(), dtype=int64, numpy=1>}\n",
      "{'language': <tf.Tensor: shape=(), dtype=string, numpy=b'de'>, 'product_category': <tf.Tensor: shape=(), dtype=string, numpy=b'lawn_and_garden'>, 'product_id': <tf.Tensor: shape=(), dtype=string, numpy=b'product_de_0901865'>, 'review_body': <tf.Tensor: shape=(), dtype=string, numpy=b'Kaum Saat aufgegangen.'>, 'review_id': <tf.Tensor: shape=(), dtype=string, numpy=b'de_0232738'>, 'review_title': <tf.Tensor: shape=(), dtype=string, numpy=b'Schlechte Mischung'>, 'reviewer_id': <tf.Tensor: shape=(), dtype=string, numpy=b'reviewer_de_0033281'>, 'stars': <tf.Tensor: shape=(), dtype=int64, numpy=1>}\n"
     ]
    }
   ],
   "source": [
    "for parsed_record in parsed_dataset.take(5):\n",
    "  print(repr(parsed_record))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mVOchXDZb6VZ"
   },
   "source": [
    "# Creación de un Sistema de recomendación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n_gWu4mBcAOh"
   },
   "source": [
    "## Preparación de datos y librerías\n",
    "\n",
    "Utilizaremos la librería TensorFlow Recomenders que esta especializada en sistemas de recomendación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "rEM1oHEOeBz3"
   },
   "outputs": [],
   "source": [
    "!pip install -q tensorflow-recommenders\n",
    "!pip install -q --upgrade tensorflow-datasets\n",
    "!pip install -q scann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "f7WX1KAyedE3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "import tempfile\n",
    "\n",
    "from typing import Dict, Text\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSuNQxePeo_D"
   },
   "source": [
    "De nuestro análisis anterior (en Spark) obtuvimos que nuestras variables a utilizar son: _product_id_,_reviewer_id_, _stars_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BArzJrf5fi5q",
    "outputId": "4482bb78-c74c-4b06-f318-af66c30a57d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'product_id': b'product_de_0678997',\n",
      " 'reviewer_id': b'reviewer_de_0783625',\n",
      " 'stars': 1}\n"
     ]
    }
   ],
   "source": [
    "parsed_dataset = parsed_dataset.map(lambda x: {\n",
    "    'reviewer_id':x['reviewer_id'], \n",
    "    'product_id':x['product_id'], \n",
    "    'stars': x['stars']\n",
    "  })\n",
    "\n",
    "\n",
    "for x in parsed_dataset.take(1).as_numpy_iterator():\n",
    "  pprint.pprint(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "tDA7xw9ofsEk"
   },
   "outputs": [],
   "source": [
    "train = parsed_dataset\n",
    "test = parsed_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xhYoJBL7uXS3",
    "outputId": "555e2bb6-15bc-4320-9e04-4a52d82762b3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: {language: (), product_category: (), product_id: (), review_body: (), review_id: (), review_title: (), reviewer_id: (), stars: ()}, types: {language: tf.string, product_category: tf.string, product_id: tf.string, review_body: tf.string, review_id: tf.string, review_title: tf.string, reviewer_id: tf.string, stars: tf.int64}>"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FCP5AUZHiTgz",
    "outputId": "c7d61d2b-6539-434d-f936-c0ab8eda499f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'product_de_0000011', b'product_de_0000029',\n",
       "       b'product_de_0000040', b'product_de_0000053',\n",
       "       b'product_de_0000060', b'product_de_0000065',\n",
       "       b'product_de_0000067', b'product_de_0000070',\n",
       "       b'product_de_0000122', b'product_de_0000130'], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products = parsed_dataset.batch(100_000).map(lambda x: x[\"product_id\"])\n",
    "user_ids = parsed_dataset.batch(100_000).map(lambda x: x[\"reviewer_id\"])\n",
    "\n",
    "unique_products = np.unique(np.concatenate(list(products)))\n",
    "unique_user_ids = np.unique(np.concatenate(list(user_ids)))\n",
    "\n",
    "unique_products[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5SX04TjDq7Pj",
    "outputId": "70ae09c4-0b46-43ea-af4d-5ecebf159b0f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'reviewer_de_0000000', b'reviewer_de_0000013',\n",
       "       b'reviewer_de_0000058', b'reviewer_de_0000088',\n",
       "       b'reviewer_de_0000098', b'reviewer_de_0000105',\n",
       "       b'reviewer_de_0000109', b'reviewer_de_0000122',\n",
       "       b'reviewer_de_0000129', b'reviewer_de_0000144'], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_user_ids[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CcHD9d19jqbR"
   },
   "source": [
    "Con esto finalizamos la preparación de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZoX2juDmjuHq"
   },
   "source": [
    "## Modelado\n",
    "\n",
    "Siguiendo la documentación de tensorflow para la implementación del modelo realizamos los siguientes pasos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "hB2CyzTckC5r"
   },
   "outputs": [],
   "source": [
    "embedding_dimension = 32 #hay que probar valores que mejor se ajusten para evitar overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCorACywk-GV"
   },
   "source": [
    "Basado en la documentación de tfrs, sabemos que podemos generar un modelo heredando de la clase model, entonces realizamos los siguientes ajustes. Primero creamos un modelo que hereda de la clase keras.Model de tensorflow en esta clase se transforman los datos de texto a numericos y se calcula la predicción, normalizando los datos aunque para estos no se hace tan necesario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "mL5f_vVTkrl7"
   },
   "outputs": [],
   "source": [
    "class RankingModel(tf.keras.Model):\n",
    "\n",
    "  def __init__(self, embedding_dimension):\n",
    "    super().__init__()\n",
    "    embedding_dimension = embedding_dimension\n",
    "\n",
    "    # Compute embeddings for users.\n",
    "    self.user_embeddings = tf.keras.Sequential([\n",
    "      tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "        vocabulary=unique_user_ids, mask_token=None),\n",
    "      tf.keras.layers.Embedding(len(unique_user_ids) + 1, embedding_dimension)\n",
    "    ])\n",
    "\n",
    "    # Compute embeddings for movies.\n",
    "    self.product_embeddings = tf.keras.Sequential([\n",
    "      tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "        vocabulary=unique_products, mask_token=None),\n",
    "      tf.keras.layers.Embedding(len(unique_products) + 1, embedding_dimension)\n",
    "    ])\n",
    "\n",
    "    # Compute predictions.\n",
    "    self.ratings = tf.keras.Sequential([\n",
    "      tf.keras.layers.experimental.preprocessing.Normalization(),\n",
    "      # Learn multiple dense layers.\n",
    "      tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "      tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "      # Make rating predictions in the final layer.\n",
    "      tf.keras.layers.Dense(1)\n",
    "  ])\n",
    "\n",
    "  def call(self, inputs):\n",
    "\n",
    "    user_id, product_id = inputs\n",
    "\n",
    "    user_embedding = self.user_embeddings(user_id)\n",
    "    product_embedding = self.product_embeddings(product_id)\n",
    "\n",
    "    return self.ratings(tf.concat([user_embedding, product_embedding], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-Z5wWk64mv2s",
    "outputId": "22771ecd-0c64-491e-dd4d-3f65e8b92059"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'list'> input: ['product_de_0000011']\n",
      "Consider rewriting this model with the Functional API.\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'list'> input: [\"'reviewer_de_0000098'\"]\n",
      "Consider rewriting this model with the Functional API.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[-0.02045668]], dtype=float32)>"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RankingModel(embedding_dimension)(([\"product_de_0000011\"], [\"'reviewer_de_0000098'\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "VEhpM3v4tfjH"
   },
   "outputs": [],
   "source": [
    "class AWSRatingsModel(tfrs.models.Model):\n",
    "\n",
    "  def __init__(self, embedding_dimension):\n",
    "    super().__init__()\n",
    "    self.ranking_model: tf.keras.Model = RankingModel(embedding_dimension)\n",
    "    self.task: tf.keras.layers.Layer = tfrs.tasks.Ranking(\n",
    "      loss = tf.keras.losses.MeanSquaredError(),\n",
    "      metrics=[tf.keras.metrics.RootMeanSquaredError()]\n",
    "    )\n",
    "\n",
    "  def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
    "    rating_predictions = self.ranking_model(\n",
    "        (features[\"reviewer_id\"], features[\"product_id\"]))\n",
    "\n",
    "    # The task computes the loss and the metrics.\n",
    "    return self.task(labels=features[\"stars\"], predictions=rating_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bO24sLta7sao"
   },
   "source": [
    "# Ejecución del modelo\n",
    "\n",
    "Instanciamos la clase del modelo que acabamos de crear pasandole de parámetro los embeddings que se ajusto sobre el modelo de guía de la documentación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "nmlfNFwdt7Wd"
   },
   "outputs": [],
   "source": [
    "model = AWSRatingsModel(embedding_dimension)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "y5eotwjTuIo3"
   },
   "outputs": [],
   "source": [
    "cached_train = train.batch(200000).cache()\n",
    "cached_test = test.batch(20000).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AVKAjDuiuNhz",
    "outputId": "bee4a65a-6cb5-49b0-9501-6e88efba338f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "2/2 [==============================] - 15s 5s/step - root_mean_squared_error: 1.4637 - loss: 2.1083 - regularization_loss: 0.0000e+00 - total_loss: 2.1083\n",
      "Epoch 2/6\n",
      "2/2 [==============================] - 2s 545ms/step - root_mean_squared_error: 1.4153 - loss: 2.0016 - regularization_loss: 0.0000e+00 - total_loss: 2.0016\n",
      "Epoch 3/6\n",
      "2/2 [==============================] - 2s 541ms/step - root_mean_squared_error: 1.4136 - loss: 1.9983 - regularization_loss: 0.0000e+00 - total_loss: 1.9983\n",
      "Epoch 4/6\n",
      "2/2 [==============================] - 2s 548ms/step - root_mean_squared_error: 1.4135 - loss: 1.9982 - regularization_loss: 0.0000e+00 - total_loss: 1.9982\n",
      "Epoch 5/6\n",
      "2/2 [==============================] - 2s 561ms/step - root_mean_squared_error: 1.4135 - loss: 1.9981 - regularization_loss: 0.0000e+00 - total_loss: 1.9981\n",
      "Epoch 6/6\n",
      "2/2 [==============================] - 2s 541ms/step - root_mean_squared_error: 1.4135 - loss: 1.9981 - regularization_loss: 0.0000e+00 - total_loss: 1.9981\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f0267626650>"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(cached_train, epochs=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJzgO4NY85Uv"
   },
   "source": [
    "## Evaluación del Modelo\n",
    "\n",
    "En este paso evaluamos llos resultados de ejecución del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XHPW-zLKu5x2",
    "outputId": "90e6ee5e-874c-42c1-c78d-3b1a87afcef2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 337ms/step - root_mean_squared_error: 1.4151 - loss: 2.0026 - regularization_loss: 0.0000e+00 - total_loss: 2.0026\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 2.0025787353515625,\n",
       " 'regularization_loss': 0,\n",
       " 'root_mean_squared_error': 1.4151250123977661,\n",
       " 'total_loss': 2.0025787353515625}"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(cached_test, return_dict=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
